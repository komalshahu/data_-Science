{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komalshahu/data_-Science/blob/main/data_science_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are your complete, structured learning notes designed to make you job-ready with Pandas for a Data Science role.\n",
        "\n",
        "-----\n",
        "\n",
        "# Mastering Pandas for Data Science: The Complete Guide\n",
        "\n",
        "This guide covers Pandas from the ground up, focusing on the practical skills, real-world data pipelines, and interview-level knowledge required for a Data Science or Data Analyst job.\n",
        "\n",
        "## 1\\. Introduction to Pandas\n",
        "\n",
        "### What is Pandas? üêº\n",
        "\n",
        "Pandas is an open-source Python library built on top of NumPy. It is the single most important tool for practical, real-world data manipulation and analysis in Python. Its name is derived from \"Panel Data,\" an econometrics term for multidimensional, structured datasets.\n",
        "\n",
        "Think of Pandas as \"Excel on steroids\" inside a Python script.\n",
        "\n",
        "### Why is it essential for Data Science?\n",
        "\n",
        "Data Science is all about data. Before you can build a machine learning model or create a visualization, your data is almost always \"dirty.\"\n",
        "\n",
        "  * **Data Wrangling:** Real-world data is messy. It has missing values, incorrect formats, and needs to be combined from multiple sources. Pandas is the tool for this *wrangling* or *munging*.\n",
        "  * **EDA (Exploratory Data Analysis):** Pandas is used to load, clean, transform, and then \"explore\" the data. You use it to summarize statistics, find patterns, and ask questions of your data.\n",
        "  * **Pipeline Integration:** It's the \"glue\" that connects your data sources (like SQL databases or CSV files) to your analysis and modeling libraries (like Scikit-learn, Statsmodels, Matplotlib, and Seaborn).\n",
        "\n",
        "### Key Data Structures\n",
        "\n",
        "1.  **Series:** A one-dimensional, labeled array (like a single column in a spreadsheet) capable of holding any data type.\n",
        "2.  **DataFrame:** A two-dimensional, labeled data structure with columns of potentially different types (like a full spreadsheet or a SQL table). It's the primary data structure you'll use 99% of the time.\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "Ma41BdwLh5m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# A Series (1D)\n",
        "s = pd.Series([10, 20, 30, np.nan], index=['a', 'b', 'c', 'd'])\n",
        "print(\"--- Series ---\")\n",
        "print(s)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Series ---\n",
            "a    10.0\n",
            "b    20.0\n",
            "c    30.0\n",
            "d     NaN\n",
            "dtype: float64\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjYTTBRGh5nz",
        "outputId": "e50f6bf4-8506-4d86-fae1-e4352a1c02c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "\n",
        "```\n",
        "--- Series ---\n",
        "a    10.0\n",
        "b    20.0\n",
        "c    30.0\n",
        "d     NaN\n",
        "dtype: float64\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## 2\\. DataFrames: Creation and Inspection\n",
        "\n",
        "### Creating DataFrames\n",
        "\n",
        "You can create DataFrames from almost any data source."
      ],
      "metadata": {
        "id": "1Z6eoeRbh5oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. From a Dictionary (Most common for small examples)\n",
        "data = {\n",
        "    'Name': ['Komal', 'Alice', 'Bob', 'Charlie'],\n",
        "    'Age': [20, 25, 30, 22],\n",
        "    'Department': ['AI/DS', 'Sales', 'Engineering', 'Sales'],\n",
        "    'Salary': [np.nan, 70000, 85000, 72000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"--- DataFrame from Dict ---\")\n",
        "print(df)\n",
        "\n",
        "# 2. From a CSV file (Most common in the real world)\n",
        "# Assume 'employees.csv' exists\n",
        "# df.to_csv('employees.csv', index=False) # How to create one\n",
        "df_from_csv = pd.read_csv('employees.csv')\n",
        "\n",
        "# 3. From an Excel file\n",
        "# Assume 'employees.xlsx' exists\n",
        "# df.to_excel('employees.xlsx', sheet_name='Sheet1', index=False)\n",
        "df_from_excel = pd.read_excel('employees.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "# 4. From JSON\n",
        "# Assume 'employees.json' exists\n",
        "# df.to_json('employees.json', orient='records')\n",
        "df_from_json = pd.read_json('employees.json', orient='records')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DataFrame from Dict ---\n",
            "      Name  Age   Department   Salary\n",
            "0    Komal   20        AI/DS      NaN\n",
            "1    Alice   25        Sales  70000.0\n",
            "2      Bob   30  Engineering  85000.0\n",
            "3  Charlie   22        Sales  72000.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'employees.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2555180912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Assume 'employees.csv' exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# df.to_csv('employees.csv', index=False) # How to create one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_from_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'employees.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 3. From an Excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'employees.csv'"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "oK-t2oovh5oE",
        "outputId": "562876b0-bafe-45bb-b896-6c926b911a90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (from Dict):**\n",
        "\n",
        "```\n",
        "--- DataFrame from Dict ---\n",
        "      Name  Age   Department   Salary\n",
        "0    Komal   20        AI/DS      NaN\n",
        "1    Alice   25        Sales  70000.0\n",
        "2      Bob   30  Engineering  85000.0\n",
        "3  Charlie   22        Sales  72000.0\n",
        "```\n",
        "\n",
        "### Common Attributes and Methods (Your First EDA)\n",
        "\n",
        "These are the *first commands* you run after loading any new dataset."
      ],
      "metadata": {
        "id": "DVzi9F-_h5oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame\n",
        "data = {\n",
        "    'Name': ['Komal', 'Alice', 'Bob', 'Charlie'],\n",
        "    'Age': [20, 25, 30, 22],\n",
        "    'Department': ['AI/DS', 'Sales', 'Engineering', 'Sales'],\n",
        "    'Salary': [np.nan, 70000, 85000, 72000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Look at the first 5 rows\n",
        "print(\"--- df.head() ---\")\n",
        "print(df.head())\n",
        "\n",
        "# 2. Get the dimensions (rows, columns)\n",
        "print(\"\\n--- df.shape ---\")\n",
        "print(df.shape) # Output: (4, 4)\n",
        "\n",
        "# 3. Get a concise summary (non-null counts, data types)\n",
        "# THIS IS THE MOST IMPORTANT ONE.\n",
        "print(\"\\n--- df.info() ---\")\n",
        "df.info()\n",
        "\n",
        "# 4. Get descriptive statistics for numeric columns\n",
        "print(\"\\n--- df.describe() ---\")\n",
        "print(df.describe())\n",
        "\n",
        "# 5. Get column names\n",
        "print(\"\\n--- df.columns ---\")\n",
        "print(df.columns) # Output: Index(['Name', 'Age', 'Department', 'Salary'], dtype='object')\n",
        "\n",
        "# 6. Get data types\n",
        "print(\"\\n--- df.dtypes ---\")\n",
        "print(df.dtypes)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "T953rTIzh5oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (df.info()):**\n",
        "\n",
        "```\n",
        "--- df.info() ---\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 4 entries, 0 to 3\n",
        "Data columns (total 4 columns):\n",
        " #   Column      Non-Null Count  Dtype  \n",
        "---  ------      --------------  -----  \n",
        " 0   Name        4 non-null      object\n",
        " 1   Age         4 non-null      int64  \n",
        " 2   Department  4 non-null      object\n",
        " 3   Salary      3 non-null      float64\n",
        "dtypes: float64(1), int64(1), object(2)\n",
        "memory usage: 256.0+ bytes\n",
        "```\n",
        "\n",
        "  * **Job-Ready Insight:** `df.info()` immediately tells you two critical things:\n",
        "    1.  **Missing Data:** `Salary` has \"3 non-null\" out of 4 entries, meaning one is missing.\n",
        "    2.  **Data Types:** `Name` and `Department` are `object` (string), which is expected. `Age` is `int64` and `Salary` is `float64` (because of the `NaN`), which is also correct. If 'Age' were an `object`, you'd need to clean it.\n",
        "\n",
        "-----\n",
        "\n",
        "## 3\\. Indexing, Slicing, and Subsetting\n",
        "\n",
        "This is how you select specific pieces of your data.\n",
        "\n",
        "### Selecting Columns"
      ],
      "metadata": {
        "id": "0UE4Ztwuh5oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a single column (returns a Series)\n",
        "print(\"--- Selecting 'Name' column (Series) ---\")\n",
        "print(df['Name'])\n",
        "\n",
        "# Select multiple columns (returns a DataFrame)\n",
        "print(\"\\n--- Selecting 'Name' and 'Salary' (DataFrame) ---\")\n",
        "print(df[['Name', 'Salary']])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-omKNOqGh5oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `.loc[]`: Label-Based Indexing\n",
        "\n",
        "Used for selecting data by **row and column labels (names)**.\n",
        "**Syntax:** `df.loc[row_labels, column_labels]`"
      ],
      "metadata": {
        "id": "TVvnu0Z_h5oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 'Name' as the index to make labels meaningful\n",
        "df_indexed = df.set_index('Name')\n",
        "print(\"--- Indexed DataFrame ---\")\n",
        "print(df_indexed)\n",
        "\n",
        "# Select row 'Alice'\n",
        "print(\"\\n--- .loc['Alice'] ---\")\n",
        "print(df_indexed.loc['Alice'])\n",
        "\n",
        "# Select rows 'Alice' and 'Bob', columns 'Age' and 'Salary'\n",
        "print(\"\\n--- .loc[['Alice', 'Bob'], ['Age', 'Salary']] ---\")\n",
        "print(df_indexed.loc[['Alice', 'Bob'], ['Age', 'Salary']])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qr77_4Mjh5oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (Indexed DataFrame):**\n",
        "\n",
        "```\n",
        "--- Indexed DataFrame ---\n",
        "         Age   Department   Salary\n",
        "Name                             \n",
        "Komal     20        AI/DS      NaN\n",
        "Alice     25        Sales  70000.0\n",
        "Bob       30  Engineering  85000.0\n",
        "Charlie   22        Sales  72000.0\n",
        "```\n",
        "\n",
        "**Output (.loc[['Alice', 'Bob'], ['Age', 'Salary']]):**\n",
        "\n",
        "```\n",
        "--- .loc[['Alice', 'Bob'], ['Age', 'Salary']] ---\n",
        "       Age   Salary\n",
        "Name               \n",
        "Alice   25  70000.0\n",
        "Bob     30  85000.0\n",
        "```\n",
        "\n",
        "### `.iloc[]`: Integer-Position-Based Indexing\n",
        "\n",
        "Used for selecting data by **row and column integer position (0-indexed)**.\n",
        "**Syntax:** `df.iloc[row_indices, column_indices]`\n",
        "*Slicing (`:`) is exclusive of the end, just like Python lists.*"
      ],
      "metadata": {
        "id": "iLqzfASwh5oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the first row (index 0)\n",
        "print(\"--- .iloc[0] ---\")\n",
        "print(df.iloc[0])\n",
        "\n",
        "# Select rows 1 and 2 (positions 1 and 2)\n",
        "print(\"\\n--- .iloc[[1, 2]] ---\")\n",
        "print(df.iloc[[1, 2]])\n",
        "\n",
        "# Select the first two rows and first two columns\n",
        "# Rows 0, 1 (2 is exclusive)\n",
        "# Cols 0, 1 (2 is exclusive)\n",
        "print(\"\\n--- .iloc[0:2, 0:2] ---\")\n",
        "print(df.iloc[0:2, 0:2])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "og6PJV2eh5oR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (.iloc[0:2, 0:2]):**\n",
        "\n",
        "```\n",
        "--- .iloc[0:2, 0:2] ---\n",
        "    Name  Age\n",
        "0  Komal   20\n",
        "1  Alice   25\n",
        "```\n",
        "\n",
        "### Boolean Indexing (The most powerful)\n",
        "\n",
        "Used to filter data based on conditions. This is a core part of data analysis."
      ],
      "metadata": {
        "id": "WV8aPkwKh5oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Condition 1: Find all employees older than 23\n",
        "print(\"--- Age > 23 ---\")\n",
        "print(df[df['Age'] > 23])\n",
        "\n",
        "# Condition 2: Find all employees in the 'Sales' department\n",
        "print(\"\\n--- Department == 'Sales' ---\")\n",
        "print(df[df['Department'] == 'Sales'])\n",
        "\n",
        "# Condition 3: Find all employees in 'Sales' AND older than 23\n",
        "# Use & (AND), | (OR), ~ (NOT)\n",
        "# Each condition MUST be in parentheses ()\n",
        "print(\"\\n--- 'Sales' AND Age > 23 ---\")\n",
        "print(df[(df['Department'] == 'Sales') & (df['Age'] > 23)])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zeilxa2xh5oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (Condition 3):**\n",
        "\n",
        "```\n",
        "--- 'Sales' AND Age > 23 ---\n",
        "    Name  Age Department   Salary\n",
        "1  Alice   25      Sales  70000.0\n",
        "```\n",
        "\n",
        "### Adding and Removing Columns/Rows"
      ],
      "metadata": {
        "id": "KGZ_Fs4rh5oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column\n",
        "df['Years_of_Exp'] = [1, 3, 8, 2]\n",
        "print(\"--- Added 'Years_of_Exp' column ---\")\n",
        "print(df)\n",
        "\n",
        "# Add a column based on another\n",
        "df['Salary_After_Tax'] = df['Salary'] * 0.80\n",
        "print(\"\\n--- Added 'Salary_After_Tax' column ---\")\n",
        "print(df)\n",
        "\n",
        "# Remove a column\n",
        "# axis=1 means \"column\"\n",
        "# inplace=True modifies the df directly (use with caution)\n",
        "df_dropped_col = df.drop('Years_of_Exp', axis=1)\n",
        "print(\"\\n--- Dropped 'Years_of_Exp' column ---\")\n",
        "print(df_dropped_col)\n",
        "\n",
        "# Remove a row (by index label)\n",
        "# axis=0 means \"row\" (it's the default)\n",
        "df_dropped_row = df.drop(0, axis=0)\n",
        "print(\"\\n--- Dropped row 0 ---\")\n",
        "print(df_dropped_row)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "gc5UP0Kdh5oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 4\\. Data Cleaning and Preprocessing\n",
        "\n",
        "This is where 80% of your time as a data scientist is spent."
      ],
      "metadata": {
        "id": "bmf8vS3Dh5oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a messy DataFrame\n",
        "data_dirty = {\n",
        "    'Name': ['Komal S.', 'Alice', 'Bob', 'Charlie', 'Alice', 'Eve '],\n",
        "    'Age': ['20', 25, 30, '22', 25, '35 '],\n",
        "    'Department': ['AI/DS', 'Sales', 'Engineering', 'Sales', 'Sales', ' admin'],\n",
        "    'Salary': [np.nan, 70000, 85000, 72000, 70000, 50000],\n",
        "    'Hire_Date': ['2024-01-15', '2022-05-20', '2020-03-10', '2023-11-01', '2022-05-20', '2024-02-01']\n",
        "}\n",
        "df_clean = pd.DataFrame(data_dirty)\n",
        "print(\"--- Original Messy Data ---\")\n",
        "df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vQNYhBkAh5oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (Messy Data):**\n",
        "\n",
        "```\n",
        "--- Original Messy Data ---\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 6 entries, 0 to 5\n",
        "Data columns (total 5 columns):\n",
        " #   Column      Non-Null Count  Dtype  \n",
        "---  ------      --------------  -----  \n",
        " 0   Name        6 non-null      object\n",
        " 1   Age         6 non-null      object\n",
        " 2   Department  6 non-null      object\n",
        " 3   Salary      5 non-null      float64\n",
        " 4   Hire_Date   6 non-null      object\n",
        "dtypes: float64(1), object(4)\n",
        "memory usage: 368.0+ bytes\n",
        "```\n",
        "\n",
        "  * **Problems found:**\n",
        "    1.  `Salary` has a missing value.\n",
        "    2.  `Name` has a duplicate ('Alice').\n",
        "    3.  `Age` is `object` (string) type, not `int`.\n",
        "    4.  `Hire_Date` is `object`, not `datetime`.\n",
        "    5.  `Department` and `Name` have whitespace issues ('Eve ', ' admin').\n",
        "\n",
        "### Handling Missing Data (isnull, fillna, dropna)"
      ],
      "metadata": {
        "id": "ASEZvko9h5oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check for missing data\n",
        "print(\"\\n--- Missing Value Counts ---\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "# 2. Fill missing data\n",
        "# We can fill the missing Salary with the mean salary\n",
        "mean_salary = df_clean['Salary'].mean()\n",
        "df_clean['Salary'] = df_clean['Salary'].fillna(mean_salary)\n",
        "print(\"\\n--- Data after fillna() ---\")\n",
        "print(df_clean)\n",
        "\n",
        "# 3. Drop rows with missing data (if filling isn't appropriate)\n",
        "# df_clean = df_clean.dropna(subset=['Salary'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "7g2k8Zllh5oV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (Missing Value Counts):**\n",
        "\n",
        "```\n",
        "--- Missing Value Counts ---\n",
        "Name          0\n",
        "Age           0\n",
        "Department    0\n",
        "Salary        1\n",
        "Hire_Date     0\n",
        "dtype: int64\n",
        "```\n",
        "\n",
        "### Removing Duplicates"
      ],
      "metadata": {
        "id": "WQ-VbaaOh5oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check for duplicates\n",
        "print(f\"\\nTotal duplicates: {df_clean.duplicated().sum()}\") # Output: 1\n",
        "\n",
        "# 2. Drop duplicates\n",
        "df_clean = df_clean.drop_duplicates(keep='first')\n",
        "print(\"\\n--- Data after drop_duplicates() ---\")\n",
        "print(df_clean)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Lq7Y_rtlh5oV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing Data Types (astype, to\\_numeric, to\\_datetime)"
      ],
      "metadata": {
        "id": "iwuBfX1mh5oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fix 'Age' (object to int)\n",
        "# pd.to_numeric is safer than .astype() because it can handle errors\n",
        "df_clean['Age'] = pd.to_numeric(df_clean['Age'])\n",
        "print(f\"\\nAge dtype: {df_clean['Age'].dtype}\") # Output: int64\n",
        "\n",
        "# 2. Fix 'Hire_Date' (object to datetime)\n",
        "df_clean['Hire_Date'] = pd.to_datetime(df_clean['Hire_Date'])\n",
        "print(f\"Hire_Date dtype: {df_clean['Hire_Date'].dtype}\") # Output: datetime64[ns]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "HgcnvQc-h5oW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### String Operations (`.str` accessor)\n",
        "\n",
        "Essential for cleaning `object` columns."
      ],
      "metadata": {
        "id": "drlbDCYFh5oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fix 'Department' and 'Name' whitespace\n",
        "df_clean['Department'] = df_clean['Department'].str.strip() # Remove leading/trailing space\n",
        "df_clean['Name'] = df_clean['Name'].str.strip()\n",
        "\n",
        "# 2. Standardize case\n",
        "df_clean['Department'] = df_clean['Department'].str.lower()\n",
        "print(\"\\n--- Data after .str.strip() and .str.lower() ---\")\n",
        "print(df_clean['Department'].values)\n",
        "# Output: ['ai/ds' 'sales' 'engineering' 'sales' 'admin']\n",
        "\n",
        "# 3. Replace values\n",
        "df_clean['Name'] = df_clean['Name'].str.replace('Komal S.', 'Komal', regex=False)\n",
        "print(f\"\\nName 'Komal S.' fixed: {'Komal' in df_clean['Name'].values}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "yrxkdhF4h5oW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying Functions (apply, map, lambda)\n",
        "\n",
        "  * `map()`: Element-wise transformation on a **Series**, using a dictionary.\n",
        "  * `apply()`: Apply a function along an axis of a **DataFrame** (or a Series).\n",
        "  * `lambda`: A small, anonymous function, often used inside `apply`.\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "mD2RXaK9h5oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Use map() to encode categorical data\n",
        "dept_map = {'ai/ds': 0, 'sales': 1, 'engineering': 2, 'admin': 3}\n",
        "df_clean['Dept_Code'] = df_clean['Department'].map(dept_map)\n",
        "print(\"\\n--- Data after map() ---\")\n",
        "print(df_clean[['Department', 'Dept_Code']])\n",
        "\n",
        "# 2. Use apply() with a lambda to create a new column\n",
        "# Create an 'Experience_Level' based on Salary\n",
        "def get_exp_level(salary):\n",
        "    if salary > 75000:\n",
        "        return 'Senior'\n",
        "    elif salary > 60000:\n",
        "        return 'Mid'\n",
        "    else:\n",
        "        return 'Junior'\n",
        "\n",
        "df_clean['Exp_Level'] = df_clean['Salary'].apply(get_exp_level)\n",
        "\n",
        "# Same thing using a lambda function (more common)\n",
        "df_clean['Exp_Level_Lambda'] = df_clean['Salary'].apply(lambda x: 'Senior' if x > 75000 else ('Mid' if x > 60000 else 'Junior'))\n",
        "print(\"\\n--- Data after apply() ---\")\n",
        "print(df_clean[['Salary', 'Exp_Level', 'Exp_Level_Lambda']])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "a71IflwIh5oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 5\\. Grouping and Aggregation\n",
        "\n",
        "This is how you summarize data‚Äîa cornerstone of business intelligence and analysis. The paradigm is **Split-Apply-Combine**.\n",
        "\n",
        "1.  **Split:** `groupby()` splits the data into groups based on criteria.\n",
        "2.  **Apply:** An aggregation function (like `sum`, `mean`, `count`) is applied to each group.\n",
        "3.  **Combine:** The results are combined into a new DataFrame.\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "47qjg01ph5oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the clean DataFrame\n",
        "print(\"--- Cleaned DataFrame for GroupBy ---\")\n",
        "print(df_clean)\n",
        "\n",
        "# 1. Group by 'Department' and find the mean salary\n",
        "print(\"\\n--- Mean Salary per Department ---\")\n",
        "dept_salary = df_clean.groupby('Department')['Salary'].mean().sort_values(ascending=False)\n",
        "print(dept_salary)\n",
        "\n",
        "# 2. Multi-level grouping\n",
        "# Group by 'Department' and 'Exp_Level' and count employees\n",
        "print(\"\\n--- Count per Department and Exp_Level ---\")\n",
        "dept_counts = df_clean.groupby(['Department', 'Exp_Level'])['Name'].count()\n",
        "print(dept_counts)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "A1Wm5It-h5oY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (Mean Salary):**\n",
        "\n",
        "```\n",
        "--- Mean Salary per Department ---\n",
        "Department\n",
        "engineering    85000.0\n",
        "sales          71000.0\n",
        "admin          50000.0\n",
        "ai/ds          69400.0  <- This is the mean_salary we filled earlier\n",
        "Name: Salary, dtype: float64\n",
        "```\n",
        "\n",
        "### `.agg()`: Multiple Aggregations\n",
        "\n",
        "The most powerful aggregation method."
      ],
      "metadata": {
        "id": "fkcm_SBVh5oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Use .agg() for multiple, custom aggregations\n",
        "print(\"\\n--- Multiple Aggregations with .agg() ---\")\n",
        "dept_summary = df_clean.groupby('Department').agg(\n",
        "    Avg_Salary=('Salary', 'mean'),\n",
        "    Max_Age=('Age', 'max'),\n",
        "    Num_Employees=('Name', 'count')\n",
        ").reset_index() # .reset_index() flattens the result back into a clean DataFrame\n",
        "\n",
        "print(dept_summary)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "u6swJYNDh5oY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (.agg()):**\n",
        "\n",
        "```\n",
        "--- Multiple Aggregations with .agg() ---\n",
        "    Department  Avg_Salary  Max_Age  Num_Employees\n",
        "0        admin     50000.0       35              1\n",
        "1        ai/ds     69400.0       20              1\n",
        "2  engineering     85000.0       30              1\n",
        "3        sales     71000.0       25              2\n",
        "```\n",
        "\n",
        "### `.transform()`: Group-level Operations (Interview Favorite)\n",
        "\n",
        "`transform` performs a group calculation but returns a result with the **same index as the original DataFrame**. This is useful for comparing an individual row to its group's property."
      ],
      "metadata": {
        "id": "xphEOvNBh5oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Use .transform() to see each employee's salary vs. their department average\n",
        "print(\"\\n--- .transform() for group-level comparison ---\")\n",
        "df_clean['Dept_Avg_Salary'] = df_clean.groupby('Department')['Salary'].transform('mean')\n",
        "df_clean['Salary_vs_Dept_Avg'] = df_clean['Salary'] - df_clean['Dept_Avg_Salary']\n",
        "\n",
        "print(df_clean[['Name', 'Department', 'Salary', 'Dept_Avg_Salary', 'Salary_vs_Dept_Avg']])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "cAnMeA7Jh5oZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (.transform()):**\n",
        "\n",
        "```\n",
        "--- .transform() for group-level comparison ---\n",
        "      Name   Department   Salary  Dept_Avg_Salary  Salary_vs_Dept_Avg\n",
        "0    Komal        ai/ds  69400.0          69400.0                 0.0\n",
        "1    Alice        sales  70000.0          71000.0             -1000.0\n",
        "2      Bob  engineering  85000.0          85000.0                 0.0\n",
        "3  Charlie        sales  72000.0          71000.0              1000.0\n",
        "5      Eve        admin  50000.0          50000.0                 0.0\n",
        "```\n",
        "\n",
        "### Pivot Tables and Crosstab\n",
        "\n",
        "  * `pd.pivot_table()`: Like in Excel. Great for summarizing data in a grid.\n",
        "  * `pd.crosstab()`: Computes a frequency table of two or more factors.\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "FfGG36jOh5oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Pivot Table\n",
        "# Index = 'Department', Columns = 'Exp_Level', Values = 'Salary'\n",
        "print(\"\\n--- Pivot Table (Avg Salary) ---\")\n",
        "pivot = pd.pivot_table(\n",
        "    df_clean,\n",
        "    values='Salary',\n",
        "    index='Department',\n",
        "    columns='Exp_Level_Lambda',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "print(pivot)\n",
        "\n",
        "# 6. Crosstab\n",
        "# How many people from each Dept are in each Exp_Level?\n",
        "print(\"\\n--- Crosstab (Counts) ---\")\n",
        "cross = pd.crosstab(df_clean['Department'], df_clean['Exp_Level_Lambda'])\n",
        "print(cross)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "JV3q0Q-xh5oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 6\\. Merging, Joining, and Concatenation\n",
        "\n",
        "This is how you combine data from different sources.\n",
        "\n",
        "  * **`concat()`:** Stacks DataFrames on top of each other (vertical, `axis=0`) or side-by-side (horizontal, `axis=1`).\n",
        "  * **`merge()`:** SQL-style joins. Combines DataFrames based on a common key/column.\n",
        "  * **`join()`:** A simpler method that joins DataFrames on their *index*.\n",
        "\n",
        "### `concat()`: Stacking Data"
      ],
      "metadata": {
        "id": "XNi1H3hdh5oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_a = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})\n",
        "df_b = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']})\n",
        "\n",
        "# Stack vertically (axis=0 is default)\n",
        "vertical_concat = pd.concat([df_a, df_b], ignore_index=True)\n",
        "print(\"--- Vertical Concat ---\")\n",
        "print(vertical_concat)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vOmI0gFUh5ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `merge()`: SQL-Style Joins (Most Important)\n",
        "\n",
        "Let's create two DataFrames: `employees` and `departments`."
      ],
      "metadata": {
        "id": "X_-lacxch5ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp = pd.DataFrame({\n",
        "    'employee_id': [101, 102, 103, 104],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'dept_id': [1, 2, 1, 3]\n",
        "})\n",
        "\n",
        "df_dept = pd.DataFrame({\n",
        "    'dept_id': [1, 2, 4],\n",
        "    'dept_name': ['Sales', 'Engineering', 'Marketing']\n",
        "})\n",
        "\n",
        "print(\"--- Employees ---\")\n",
        "print(df_emp)\n",
        "print(\"\\n--- Departments ---\")\n",
        "print(df_dept)\n",
        "\n",
        "# 1. Inner Join (Default)\n",
        "# Only keeps rows where the key ('dept_id') exists in BOTH tables.\n",
        "# David (3) and Marketing (4) are dropped.\n",
        "inner_join = pd.merge(df_emp, df_dept, on='dept_id', how='inner')\n",
        "print(\"\\n--- Inner Join ---\")\n",
        "print(inner_join)\n",
        "\n",
        "# 2. Left Join\n",
        "# Keeps all rows from the 'left' table (df_emp) and matches where it can.\n",
        "# David (3) is kept, but 'dept_name' is NaN.\n",
        "left_join = pd.merge(df_emp, df_dept, on='dept_id', how='left')\n",
        "print(\"\\n--- Left Join ---\")\n",
        "print(left_join)\n",
        "\n",
        "# 3. Right Join\n",
        "# Keeps all rows from the 'right' table (df_dept) and matches where it can.\n",
        "# Marketing (4) is kept, but employee fields are NaN.\n",
        "right_join = pd.merge(df_emp, df_dept, on='dept_id', how='right')\n",
        "print(\"\\n--- Right Join ---\")\n",
        "print(right_join)\n",
        "\n",
        "# 4. Outer Join\n",
        "# Keeps all rows from BOTH tables.\n",
        "# David (3) and Marketing (4) are both kept, with NaNs in their non-matching fields.\n",
        "outer_join = pd.merge(df_emp, df_dept, on='dept_id', how='outer')\n",
        "print(\"\\n--- Outer Join ---\")\n",
        "print(outer_join)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "mfVbBPTJh5ol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 7\\. Real-World Data Analysis Example (Mini-Project)\n",
        "\n",
        "**Goal:** Analyze a \"Sales\" dataset to find the total revenue per product category.\n",
        "\n",
        "**Data:** We have two datasets:\n",
        "\n",
        "1.  `sales.csv`: Contains transaction records.\n",
        "2.  `products.csv`: Contains product details.\n",
        "\n",
        "**Pipeline:** **Load -\\> Clean -\\> Merge -\\> Feature Engineer -\\> Aggregate -\\> Analyze**"
      ],
      "metadata": {
        "id": "xFi5l6Q6h5om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. LOAD: Create sample data\n",
        "sales_data = {\n",
        "    'transaction_id': [1, 2, 3, 4, 5, 6],\n",
        "    'product_id': [10, 20, 10, 30, 20, 40],\n",
        "    'quantity': [2, 1, 1, 5, 3, 1],\n",
        "    'sale_date': ['2025-10-01', '2025-10-01', '2025-10-02', '2025-10-02', '2025-10-03', '2025-10-03']\n",
        "}\n",
        "df_sales = pd.DataFrame(sales_data)\n",
        "\n",
        "products_data = {\n",
        "    'product_id': [10, 20, 30],\n",
        "    'product_name': ['Laptop', 'Mouse', 'Keyboard'],\n",
        "    'category': ['Electronics', 'Electronics', 'Electronics'],\n",
        "    'price': [1200, 40, 100]\n",
        "}\n",
        "df_products = pd.DataFrame(products_data)\n",
        "\n",
        "print(\"--- Sales Data ---\")\n",
        "print(df_sales)\n",
        "print(\"\\n--- Products Data ---\")\n",
        "print(df_products)\n",
        "\n",
        "# 2. CLEAN: Check for issues\n",
        "# df_sales.info() -> No missing data.\n",
        "# df_products.info() -> No missing data.\n",
        "# Notice 'product_id' 40 is in sales but not products. A left join will be important.\n",
        "\n",
        "# 3. MERGE: Combine sales with product details\n",
        "# We use a 'left' join to keep all sales, even if the product is unknown.\n",
        "df_merged = pd.merge(\n",
        "    df_sales,\n",
        "    df_products,\n",
        "    on='product_id',\n",
        "    how='left'\n",
        ")\n",
        "print(\"\\n--- Merged Data ---\")\n",
        "print(df_merged)\n",
        "\n",
        "# 4. FEATURE ENGINEERING: Create the 'Revenue' column\n",
        "df_merged['revenue'] = df_merged['quantity'] * df_merged['price']\n",
        "print(\"\\n--- Merged Data with 'Revenue' ---\")\n",
        "print(df_merged)\n",
        "\n",
        "# 5. AGGREGATE: Find total revenue per category\n",
        "# We should fillna for unknown categories\n",
        "df_merged['category'] = df_merged['category'].fillna('Unknown')\n",
        "\n",
        "category_revenue = df_merged.groupby('category')['revenue'].sum().reset_index()\n",
        "\n",
        "# 6. ANALYZE: Sort to find the top category\n",
        "final_report = category_revenue.sort_values(by='revenue', ascending=False)\n",
        "print(\"\\n--- FINAL REPORT: Revenue by Category ---\")\n",
        "print(final_report)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "StjkgdJJh5om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (Final Report):**\n",
        "\n",
        "```\n",
        "--- FINAL REPORT: Revenue by Category ---\n",
        "      category  revenue\n",
        "0  Electronics   2860.0\n",
        "1      Unknown      NaN  <- Problem!\n",
        "```\n",
        "\n",
        "  * **Job-Ready Refinement:** The 'Unknown' category has `NaN` revenue because its price was `NaN`. We must handle this in the cleaning step.\n",
        "  * **Revised Pipeline (Step 4.5):**\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "hdzgt_hHh5on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix: Fill missing prices *before* calculating revenue\n",
        "df_merged['price'] = df_merged['price'].fillna(0) # Assume 0 price for unknown products\n",
        "df_merged['revenue'] = df_merged['quantity'] * df_merged['price']\n",
        "df_merged['category'] = df_merged['category'].fillna('Unknown')\n",
        "\n",
        "# Re-run aggregation\n",
        "category_revenue = df_merged.groupby('category')['revenue'].sum().reset_index()\n",
        "final_report = category_revenue.sort_values(by='revenue', ascending=False)\n",
        "\n",
        "print(\"\\n--- FINAL REPORT (REVISED) ---\")\n",
        "print(final_report)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1ETIi8bVh5on"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output (Final Report Revised):**\n",
        "\n",
        "```\n",
        "--- FINAL REPORT (REVISED) ---\n",
        "      category  revenue\n",
        "0  Electronics   2860.0\n",
        "1      Unknown      0.0\n",
        "```\n",
        "\n",
        "This demonstrates the iterative nature of data cleaning and analysis.\n",
        "\n",
        "-----\n",
        "\n",
        "## 8\\. Common Interview Questions (Conceptual & Coding)\n",
        "\n",
        "### Conceptual Questions\n",
        "\n",
        "1.  **What is the difference between `.loc[]` and `.iloc[]`?**\n",
        "\n",
        "      * **`.loc[]`** is label-based. It selects data based on index labels and column names. Slicing (`df.loc['a':'c']`) is *inclusive* of the end.\n",
        "      * **`.iloc[]`** is integer-based. It selects data based on integer positions (0-indexed). Slicing (`df.iloc[0:3]`) is *exclusive* of the end, just like Python lists.\n",
        "\n",
        "2.  **Explain `merge()`, `join()`, and `concat()`.**\n",
        "\n",
        "      * **`concat()`** is for stacking‚Äîgluing DataFrames together vertically (`axis=0`) or horizontally (`axis=1`).\n",
        "      * **`merge()`** is the primary, most flexible method for SQL-style joins (inner, left, right, outer) based on one or more common columns (keys).\n",
        "      * **`join()`** is a simpler method that merges DataFrames based on their *index labels* rather than columns.\n",
        "\n",
        "3.  **What is the difference between `apply()`, `map()`, and `transform()`?**\n",
        "\n",
        "      * **`map()`** is a Series method for element-wise substitution. It's best used with a dictionary to change values in a single column (e.g., `df['col'].map({'Yes': 1, 'No': 0})`).\n",
        "      * **`apply()`** is more flexible. It can be used on a Series (like `map`) or on a DataFrame to apply a function to each row (`axis=1`) or column (`axis=0`).\n",
        "      * **`transform()`** is a GroupBy method. It performs a group-level calculation (like `mean`) but then *broadcasts* the result back to every row in the original group, maintaining the original DataFrame's shape.\n",
        "\n",
        "4.  **How do you handle missing data?**\n",
        "\n",
        "      * **Identify:** First, use `df.isnull().sum()` to find which columns have missing values.\n",
        "      * **Drop:** If the missing data is small and random, you can drop the rows (`df.dropna()`) or columns (`df.drop(col, axis=1)`).\n",
        "      * **Impute:** This is usually better. You can fill the missing values using `df.fillna()` with a static value (like 0), or a calculated value (like the column's mean or median: `df['col'].fillna(df['col'].mean())`).\n",
        "\n",
        "### Coding Questions"
      ],
      "metadata": {
        "id": "nbeZb-9Oh5on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for coding questions\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily', 'Frank'],\n",
        "    'Department': ['Sales', 'Engineering', 'Sales', 'Engineering', 'HR', 'HR'],\n",
        "    'Salary': [70000, 85000, 72000, 90000, 60000, 58000],\n",
        "    'Age': [25, 30, 22, 35, 28, 40]\n",
        "}\n",
        "df_interview = pd.DataFrame(data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8rIkJeXsh5oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: Select all employees in 'Engineering' with a salary over 80,000.**"
      ],
      "metadata": {
        "id": "GJs8X8VHh5oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer1 = df_interview[\n",
        "    (df_interview['Department'] == 'Engineering') &\n",
        "    (df_interview['Salary'] > 80000)\n",
        "]\n",
        "print(answer1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "PfYkOON0h5oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: Find the average salary *and* max age for each department.**"
      ],
      "metadata": {
        "id": "2Rqwvj9Eh5oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer2 = df_interview.groupby('Department').agg(\n",
        "    Avg_Salary=('Salary', 'mean'),\n",
        "    Max_Age=('Age', 'max')\n",
        ").reset_index()\n",
        "print(answer2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "GI7GvFGlh5op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3: Create a new column 'Age\\_Group' with categories '20-29', '30-39', '40+'.**"
      ],
      "metadata": {
        "id": "rqV13-9Zh5op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pd.cut(), the professional way\n",
        "bins = [19, 29, 39, np.inf]\n",
        "labels = ['20-29', '30-39', '40+']\n",
        "df_interview['Age_Group'] = pd.cut(df_interview['Age'], bins=bins, labels=labels, right=True)\n",
        "print(df_interview[['Name', 'Age', 'Age_Group']])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8c4zkYsWh5op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4 (Advanced): Find the top 2 highest-paid employees in *each* department.**"
      ],
      "metadata": {
        "id": "mbgajv_xh5op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer4 = df_interview.groupby('Department', group_keys=False) \\\n",
        "                      .apply(lambda x: x.nlargest(2, 'Salary')) \\\n",
        "                      .reset_index(drop=True)\n",
        "print(answer4)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jr9ZKwgEh5oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Explanation:**\n",
        "    1.  `groupby('Department')` splits the data.\n",
        "    2.  `.apply(lambda x: ...)` applies a function to *each* department's mini-DataFrame (`x`).\n",
        "    3.  `x.nlargest(2, 'Salary')` finds the 2 rows with the largest 'Salary' *within that group*.\n",
        "    4.  `group_keys=False` prevents `groupby` from adding an extra multi-index.\n",
        "\n",
        "-----\n",
        "\n",
        "## 9\\. Pandas Revision Cheat Sheet (Job-Ready Focus)\n",
        "\n",
        "| Task | Function / Syntax | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **Reading Data** | `pd.read_csv()` | `df = pd.read_csv('file.csv')` |\n",
        "| | `pd.read_excel()` | `df = pd.read_excel('file.xlsx')` |\n",
        "| **Inspecting Data** | `df.info()` | `df.info()` (Check NaNs, Dtypes) |\n",
        "| | `df.describe()` | `df.describe()` (Stats for numeric cols) |\n",
        "| | `df.head()` | `df.head(10)` (See first 10 rows) |\n",
        "| | `df.shape` | `print(df.shape)` (Rows, Cols) |\n",
        "| **Selecting Data** | `df['col']` | `df['Salary']` (Single column) |\n",
        "| | `df[['col1', 'col2']]` | `df[['Name', 'Age']]` (Multiple columns) |\n",
        "| | `df.loc[]` | `df.loc[0:5, ['Name', 'Age']]` (Label-based) |\n",
        "| | `df.iloc[]` | `df.iloc[0:5, [0, 1]]` (Integer-based) |\n",
        "| | Boolean Indexing | `df[df['Age'] > 25]` |\n",
        "| **Data Cleaning** | `df.isnull().sum()` | `df.isnull().sum()` (Find all NaNs) |\n",
        "| | `df.fillna()` | `df['col'].fillna(df['col'].mean())` |\n",
        "| | `df.dropna()` | `df.dropna(subset=['col'])` |\n",
        "| | `df.drop_duplicates()`| `df.drop_duplicates(subset=['id'])` |\n",
        "| | `df.rename()` | `df.rename(columns={'old':'new'})` |\n",
        "| | `df.astype()` | `df['col'].astype(int)` |\n",
        "| | `pd.to_numeric()` | `pd.to_numeric(df['col'], errors='coerce')` |\n",
        "| | `pd.to_datetime()` | `pd.to_datetime(df['col'])` |\n",
        "| **String Ops** | `.str.strip()` | `df['col'].str.strip()` (Remove whitespace) |\n",
        "| | `.str.lower()` | `df['col'].str.lower()` (To lowercase) |\n",
        "| | `.str.replace()` | `df['col'].str.replace('$', '')` |\n",
        "| | `.str.contains()` | `df[df['col'].str.contains('http')]` |\n",
        "| **Functions** | `df.apply()` | `df.apply(my_func, axis=1)` (Row-wise) |\n",
        "| | `df['col'].map()` | `df['col'].map({'Yes':1, 'No':0})` |\n",
        "| **Grouping** | `df.groupby()` | `df.groupby('Dept')['Salary'].mean()` |\n",
        "| | `.agg()` | `df.groupby('Dept').agg(Avg=('Sal','mean'))` |\n",
        "| | `.transform()` | `df['Dept_Avg'] = df.groupby('Dept')['Sal'].transform('mean')` |\n",
        "| **Combining** | `pd.concat()` | `pd.concat([df1, df2], axis=0)` (Stack rows) |\n",
        "| | `pd.merge()` | `pd.merge(df1, df2, on='id', how='left')` |\n",
        "| **Reshaping** | `pd.pivot_table()` | `pd.pivot_table(df, values='A', index='B', cols='C')` |\n",
        "| | `df.sort_values()` | `df.sort_values(by='col', ascending=False)` |"
      ],
      "metadata": {
        "id": "zcnsG0YRh5or"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}